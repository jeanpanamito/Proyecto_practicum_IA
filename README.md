# Proyecto_practicum_IA
Aplicaci贸n de T茅cnicas de Inteligencia Artificial en Informaci贸n Relacionada a la Seguridad Nacional
# Documentaci贸n del proyecto "Aplicaci贸n de T茅cnicas de Inteligencia Artificial en Informaci贸n Relacionada a la Seguridad Nacional"

El proyecto "Aplicaci贸n de T茅cnicas de Inteligencia Artificial en Informaci贸n Relacionada a la Seguridad Nacional" utiliza varias t茅cnicas de procesamiento del lenguaje natural y an谩lisis de sentimientos para procesar y analizar datos relacionados con la seguridad nacional.

## Descripci贸n de Metadatos
Se utiliz贸 la API de Twitter para guardar en una colecci贸n de MongoDB documentos que contienen metadatos de tweets sobre Seguridad en Ecuador
Metadato | Descripci贸n |
---------| ----------- |
id | Codigo del tweet
time | Fecha y hora de extracci贸n del tweet 
created_at | Fecha y hora de creaci贸n del tweet
full_text | Texto completo del tweet
clean_full_text | Texto procesado del tweet
user | Es un objeto del usuario que contiene su nombre, ubicaci贸n, numero de seguidores
url_tweet | Url construida de acceso al tweet
place | Lugar de creaci贸n del tweet
retweet_count | Conteo de retweets
hastags | Hastags extraidos del tweet
urls | Urls extraidas del tweet
photos | Urls de im谩genes que contiene el tweet
videos | Urls de videos que contenga el tweet

## Instalaci贸n de m贸dulos
El proyecto requiere la instalaci贸n de los siguientes m贸dulos. El signo de exclamaci贸n es en caso de realizarse en un notebook. La l铆nea de c贸digo: 
`-m spacy download es_core_news_sm` descarga un modelo espec铆fico para el procesamiento del lenguaje espa帽ol (es) proporcionado por Spacy. 

```shell
!pip install pandas 
!pip install tweepy pymongo 
!pip install spacy 
!python -m spacy download es_core_news_sm #Setting adicional para Spacy
!pip install transformers
!pip install pyspellchecker
!pip install openai
```

## Importaci贸n de librer铆as
El proyecto utiliza las siguientes librer铆as:

```python
import tweepy
import nltk
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import re
import spacy
from nltk.tokenize import word_tokenize
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import numpy as np
from scipy.special import softmax
import csv
import urllib.request
from nltk.corpus import stopwords
from spellchecker import SpellChecker
```

A continuaci贸n, se detalla la funcionalidad de cada librer铆a importada:

1. `tweepy`: Proporciona una interfaz para acceder a la API de Twitter.
2. `nltk`: Es la biblioteca Natural Language Toolkit utilizada para el procesamiento del lenguaje natural.
3. `pymongo.mongo_client`: Se utiliza para conectarse a una base de datos de MongoDB.
4. `pymongo.server_api`: Se utiliza para especificar la versi贸n de la API del servidor de MongoDB.
5. `nltk.sentiment.vader`: Proporciona una herramienta para realizar an谩lisis de sentimientos utilizando el modelo VADER.
6. `re`: El m贸dulo de expresiones regulares se utiliza para realizar operaciones de b煤squeda y manipulaci贸n de cadenas de texto.
7. `nltk.tokenize.word_tokenize`: Se utiliza para dividir un texto en palabras o tokens individuales.
8. `transformers`: Es una biblioteca para usar modelos de aprendizaje autom谩tico preentrenados en el procesamiento del lenguaje natural.
9. `numpy`: Se utiliza para realizar operaciones num茅ricas y matem谩ticas eficientes.
10. `scipy.special.softmax`: Proporciona una funci贸n para calcular la funci贸n softmax en un arreglo.
11. `csv`: Se utiliza para trabajar con archivos CSV (valores separados por comas).
12. `urllib.request`: Se utiliza para abrir y leer URL.
13. `nltk.corpus.stopwords`: Proporciona una lista de palabras comunes que se pueden filtrar en el procesamiento del lenguaje natural.
14. `spellchecker`: Es una biblioteca para corregir la ortograf铆a en textos.
15. `spacy`: Biblioteca de procesamiento del lenguaje natural (PLN). Se la utilizar谩 para 

## Conexi贸n a la base de datos MongoDB
El proyecto se conecta a una base de datos MongoDB utilizando la siguiente configuraci贸n:

```python
uri = "mongodb+srv://<username>:<password>@<cluster_url>/"
client = MongoClient(uri, server_api=ServerApi('1'))
```

El URI de conexi贸n debe reemplazarse con las credenciales y la URL del cl煤ster MongoDB.

## Especificaci贸n de la colecci贸n de la base de datos
El proyecto especifica la base de datos y la colecci贸n de MongoDB con la siguiente configuraci贸n:

```python
mongo_db = 'Preprocessing'
mongo_collection = 'tweets'
db = client[mongo_db]
datos = db[mongo_collection].find
```


## [Preprocesamiento](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=omi0VcgIhMpP) {:target="_blank"}

El proyecto realiza varias etapas de preprocesamiento de datos para preparar los textos de los tweets antes de realizar el an谩lisis de sentimientos y la clasificaci贸n.


### 1. Limpieza de texto

Se realiza una serie de pasos para limpiar el texto de los tweets:

- Remover usuarios
- Remover caract茅res especiales manteniendo acentos.
- Remover URL.
- Remover puntuaciones.
- Remover espacios extra.
- Remover leading/trailing spaces.

### 2. StopWords
Se usa la librer铆a nltk para remover stopwords <br>
Se descargan los recursos para el tokenizador y las palabras vac铆as (stopwords) en espa帽ol utilizando `nltk.download('stopwords')` y `nltk.download('punkt')` <br>
Se carga la lista de palabras vac铆as en espa帽ol utilizando stopwords.words('spanish').
Se agrega manualmente la palabra 'rt' a la lista de stopwords utilizando stop_words.extend(['rt']). 'rt' generalmente se refiere a "retweet" y a menudo se elimina en an谩lisis de texto.

### 3. Tokenizaci贸n

Los tweets se dividen en palabras o tokens individuales utilizando el tokenizador `word_tokenize` de NLTK. Esto nos permite trabajar con cada palabra por separado en etapas posteriores.

### 4. Eliminaci贸n de palabras irrelevantes

Se eliminan las palabras irrelevantes, como los art铆culos, pronombres y preposiciones, utilizando la lista de palabras vac铆as (stop words) proporcionada por NLTK.

### 5. Correcci贸n ortogr谩fica

Se realiza una correcci贸n ortogr谩fica en los tweets utilizando la biblioteca `SpellChecker`. Esto ayuda a corregir posibles errores de escritura y mejorar la precisi贸n del an谩lisis de sentimientos. (No implementada)

### 6. Lemmatizaci贸n

Se realiza la lematizaci贸n de las palabras para reducir las palabras a su forma base o lema. Esto ayuda a reducir la variabilidad y mejorar la precisi贸n del an谩lisis de sentimientos.

## Modelado de t贸picos

Durante esta etapa, se pretende recuperar toda la informaci贸n obtenida desde los tweets para encontrar los temas generales y tem谩ticas implicitas que abarquen todo el contenido del texto, para de esa forma ordenar, resumir y tener mejor comprensi贸n sobre el mismo.
Para este proyecto, hemos decidido trabajar bajo el algoritmo de LDA(Latent Dirichlet Allocation).

##  An谩lisis de Sentimientos 

En la presente documentacion, se describe el proceso de an谩lisis de sentimientos realizado utilizando dos modelos de procesamiento de lenguaje natural. El objetivo fue evaluar diferentes enfoques y determinar cu谩l de los modelos proporcionaba los mejores resultados en t茅rminos de clasificaci贸n de sentimientos en textos, espec铆ficamente en tweets.

### Modelos Utilizados

Se seleccionaron los siguientes modelos para realizar el an谩lisis de sentimientos:

1. **VADER (Valence Aware Dictionary and sEntiment Reasoner):** Se trata de un analizador de sentimientos basado en reglas, incluido en la biblioteca NLTK (Natural Language Toolkit). Este modelo utiliza un conjunto de palabras y reglas predefinidas para asignar un puntaje de sentimiento a cada palabra y generar un puntaje general de sentimiento para un texto.

2. **Twitter-roBERTa-base:** Es un modelo basado en transformer, espec铆ficamente dise帽ado para tareas de an谩lisis de sentimientos en tweets. Se utiliza la biblioteca Transformers para cargar y utilizar este modelo preentrenado.

### Proceso de An谩lisis de Sentimientos

El proceso de an谩lisis de sentimientos se llev贸 a cabo de la siguiente manera:

1. **An谩lisis de Sentimientos con VADER:**

   ```python
   import nltk
   from nltk.sentiment import SentimentIntensityAnalyzer

   # Crear una instancia del analizador de sentimientos VADER
   sia = SentimentIntensityAnalyzer()

   datos = db[mongo_collection].find()

   for tweet in datos:
       tweet_text = tweet['full_text']

       # Realizar preprocesamiento del texto
       clean_text = remove_punctuation(remove_rt(remove_numbers(text_lowercase(tweet_text))))

       # Tokenizaci贸n del texto utilizando una expresi贸n regular
       tokens = re.findall(r'\w+', clean_text)

       # Etiquetar el sentimiento de cada token utilizando VADER
       sentiment_scores = [sia.polarity_scores(token)['compound'] for token in tokens]

       # Asignar el sentimiento promedio del texto al campo 'sentiment'
       tweet['sentiment'] = sum(sentiment_scores) / len(sentiment_scores)

       # Presentar los resultados
       print("Texto original:", tweet_text)
       print("Clean Text:", clean_text)
       print("Tokens:", tokens)
       print("Sentimientos:", sentiment_scores)
       print("Sentimiento promedio:", tweet['sentiment'])
       print("------------------------")
   ```

   En este proceso, se utiliz贸 el analizador de sentimientos VADER de NLTK. Se realiz贸 un preprocesamiento del texto para eliminar puntuaci贸n, n煤meros y menciones a retweets. Luego, se tokeniz贸 el texto y se asign贸 un puntaje de sentimiento a cada token utilizando VADER. El sentimiento promedio se asign贸 al campo 'sentiment' en cada documento de tweet.

2. **An谩lisis de Sentimientos con Twitter-roBERTa-base:**

   ```python
   from transformers import AutoModelForSequenceClassification
   from transformers import AutoTokenizer
   import numpy as np
   from scipy.special import softmax

   task = 'sentiment'
   MODEL = f"cardiffnlp/twitter-roberta-base-{task}"

   tokenizer = AutoTokenizer.from_pretrained(MODEL)

   # Cargar el modelo preentrenado
   model = AutoModelForSequenceClassification.from_pretrained(MODEL)
   model.save_pretrained(MODEL)

   text = "Good night "
   text = preprocess(text)
   encoded_input = tokenizer(text, return_tensors='pt')
   output = model(**encoded_input)
   scores = output[0][0].detach().numpy()
   scores = softmax(scores)

   ranking = np.argsort(scores)
   ranking = ranking[::-1]
   for i in range(scores.shape[0]):
       l = labels[ranking[i]]
       s = scores[ranking[i]]
       print(f"{i+1}) {l} {np.round(float(s), 4)}")
   ```

   En este caso, se utiliz贸 el modelo Twitter-roBERTa-base para el an谩lisis de sentimientos en tweets. Se carg贸 el modelo y se utiliz贸 un tokenizer espec铆fico para este modelo. Luego, se realiz贸 el an谩lisis de sentimientos en un texto de ejemplo y se obtuvieron las puntuaciones de sentimiento para cada etiqueta posible.

### Conclusiones

El an谩lisis de sentimientos realizado utilizando los modelos VADER y Twitter-roBERTa-base proporcion贸 resultados interesantes. El enfoque basado en reglas de VADER fue r谩pido y f谩cil de implementar, y mostr贸 una buena capacidad para identificar el sentimiento general en los tweets. Por otro lado, el modelo Twitter-roBERTa-base, basado en transformer, mostr贸 un enfoque m谩s sofisticado y ajustado espec铆ficamente para el an谩lisis de sentimientos en tweets.

En general, ambos modelos fueron 煤tiles para el an谩lisis de sentimientos en textos, pero el modelo Twitter-roBERTa-base demostr贸 una mayor precisi贸n y capacidad para capturar matices en los sentimientos expresados en los tweets. Por lo tanto, se recomienda utilizar este modelo para tareas de an谩lisis de sentimientos en tweets en situaciones donde se requiera un mayor nivel de detalle y precisi贸n.

## Clasificaci贸n de temas

El proyecto utiliza un modelo de clasificaci贸n de temas preentrenado para clasificar los tweets en categor铆as tem谩ticas. El modelo utiliza la arquitectura BERT (Bidirectional Encoder Representations from Transformers) y ha sido entrenado en un conjunto de datos etiquetados. El modelo toma el texto del tweet y lo clasifica en una de las categor铆as tem谩ticas predefinidas.

## Almacenamiento de resultados

Los resultados del an谩lisis de sentimientos y clasificaci贸n de temas se almacenan en la base de datos MongoDB. Cada documento en la colecci贸n "tweets" contiene la informaci贸n original del tweet, el sentimiento calculado y la categor铆a tem谩tica asignada.

## Conclusiones

El proyecto proporciona una manera eficiente de procesar y analizar grandes vol煤menes de datos de Twitter relacionados con la seguridad nacional. El preprocesamiento de datos y el an谩lisis de sentimientos permiten obtener informaci贸n valiosa sobre la opini贸n p煤blica y los temas relevantes en la plataforma. La clasificaci贸n de temas ayuda a organizar y categorizar los tweets, facilitando el an谩lisis y la identificaci贸n de tendencias.

El proyecto se puede adaptar y personalizar seg煤n las necesidades espec铆ficas del usuario, como agregar nuevas categor铆as tem谩ticas o entrenar modelos personalizados. Tambi茅n se pueden aplicar t茅cnicas adicionales, como la detecci贸n de entidades y la extracci贸n de caracter铆sticas, para obtener informaci贸n m谩s detallada de los tweets.

En resumen, el proyecto ofrece una soluci贸n robusta y escalable para el an谩lisis de datos de Twitter relacionados con la seguridad nacional, brindando una comprensi贸n m谩s profunda de las opiniones y temas relevantes en la plataforma.
