# Proyecto_practicum_IA
Aplicaci贸n de T茅cnicas de Inteligencia Artificial en Informaci贸n Relacionada a la Seguridad Nacional

# Documentaci贸n del proyecto "Aplicaci贸n de T茅cnicas de Inteligencia Artificial en Informaci贸n Relacionada a la Seguridad Nacional"
El proyecto "Aplicaci贸n de T茅cnicas de Inteligencia Artificial en Informaci贸n Relacionada a la Seguridad Nacional" utiliza varias t茅cnicas de procesamiento del lenguaje natural y an谩lisis de sentimientos para procesar y analizar datos relacionados con la seguridad nacional. <br>

Se utiliz贸 el siguiente [notebook](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=nADt1RBGXbDX&uniqifier=1) de Google Colab para el desarrollo del proyecto

## Tabla de Contenidos
  - [Descripci贸n de Metadatos](#descripci贸n-de-metadatos)
  - [Instalaci贸n de m贸dulos](#instalaci贸n-de-m贸dulos)
  - [Importaci贸n de librer铆as](#importaci贸n-de-librer铆as)
  - [Conexi贸n a la base de datos MongoDB](#conexi贸n-a-la-base-de-datos-mongodb)
  - [Especificaci贸n de la colecci贸n de la base de datos](#especificaci贸n-de-la-colecci贸n-de-la-base-de-datos)
  - [Preprocesamiento](#preprocesamiento)
    - [1. Discriminar Retweets](#1-discriminar-retweets)
    - [2. Filtrado por ubicaci贸n](#2-filtrado-por-ubicaci贸n)
    - [3. Limpieza de texto](#3-limpieza-de-texto)
    - [4. StopWords](#4-stopwords)
    - [5. Tokenizaci贸n](#5-tokenizaci贸n)
    - [6. Correcci贸n ortogr谩fica](#6-correcci贸n-ortogr谩fica)
    - [7. Lemmatizaci贸n](#7-lemmatizaci贸n)
    - [8. WordCloud](#8-wordcloud)
  - [Modelado de t贸picos](#modelado-de-t贸picos)
    - 
  - [An谩lisis de Sentimientos](#an谩lisis-de-sentimientos)
    - [Modelos Utilizados](#modelos-utilizados)
    - [Proceso de An谩lisis de Sentimientos](#proceso-de-an谩lisis-de-sentimientos)
    - [Conclusiones](#conclusiones)
  - [Clasificaci贸n de temas](#clasificaci贸n-de-temas)
  - [Almacenamiento de resultados](#almacenamiento-de-resultados)
  - [Conclusiones](#conclusiones-1)



## Descripci贸n de Metadatos
Se utiliz贸 la API de Twitter para guardar en una colecci贸n de MongoDB documentos que contienen metadatos de tweets sobre Seguridad en Ecuador
Metadato | Descripci贸n |
---------| ----------- |
id | Codigo del tweet
time | Fecha y hora de extracci贸n del tweet 
created_at | Fecha y hora de creaci贸n del tweet
full_text | Texto completo del tweet
clean_full_text | Texto procesado del tweet
user | Es un objeto del usuario que contiene su nombre, ubicaci贸n, numero de seguidores
url_tweet | Url construida de acceso al tweet
place | Lugar de creaci贸n del tweet
retweet_count | Conteo de retweets
hastags | Hastags extraidos del tweet
urls | Urls extraidas del tweet
photos | Urls de im谩genes que contiene el tweet
videos | Urls de videos que contenga el tweet

## [Instalaci贸n de m贸dulos](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=O6QlgIXNOkJO)
El proyecto requiere la instalaci贸n de los siguientes m贸dulos. El signo de exclamaci贸n es en caso de realizarse en un notebook. La l铆nea de c贸digo: 
`-m spacy download es_core_news_sm` descarga un modelo espec铆fico para el procesamiento del lenguaje espa帽ol (es) proporcionado por Spacy. 

```shell
!pip install pandas 
!pip install tweepy pymongo 
!pip install spacy 
!python -m spacy download es_core_news_sm #Setting adicional para Spacy
!pip install transformers
!pip install pyspellchecker
!pip install openai
```

## Importaci贸n de librer铆as
El proyecto utiliza las siguientes librer铆as:

```python
import tweepy
import nltk
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import re
import spacy
from nltk.tokenize import word_tokenize
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import numpy as np
from scipy.special import softmax
import csv
import urllib.request
from nltk.corpus import stopwords
from spellchecker import SpellChecker
```

A continuaci贸n, se detalla la funcionalidad de cada librer铆a importada:

1. `tweepy`: Proporciona una interfaz para acceder a la API de Twitter.
2. `nltk`: Es la biblioteca Natural Language Toolkit utilizada para el procesamiento del lenguaje natural.
3. `pymongo.mongo_client`: Se utiliza para conectarse a una base de datos de MongoDB.
4. `pymongo.server_api`: Se utiliza para especificar la versi贸n de la API del servidor de MongoDB.
5. `nltk.sentiment.vader`: Proporciona una herramienta para realizar an谩lisis de sentimientos utilizando el modelo VADER.
6. `re`: El m贸dulo de expresiones regulares se utiliza para realizar operaciones de b煤squeda y manipulaci贸n de cadenas de texto.
7. `nltk.tokenize.word_tokenize`: Se utiliza para dividir un texto en palabras o tokens individuales.
8. `transformers`: Es una biblioteca para usar modelos de aprendizaje autom谩tico preentrenados en el procesamiento del lenguaje natural.
9. `numpy`: Se utiliza para realizar operaciones num茅ricas y matem谩ticas eficientes.
10. `scipy.special.softmax`: Proporciona una funci贸n para calcular la funci贸n softmax en un arreglo.
11. `csv`: Se utiliza para trabajar con archivos CSV (valores separados por comas).
12. `urllib.request`: Se utiliza para abrir y leer URL.
13. `nltk.corpus.stopwords`: Proporciona una lista de palabras comunes que se pueden filtrar en el procesamiento del lenguaje natural.
14. `spellchecker`: Es una biblioteca para corregir la ortograf铆a en textos.
15. `spacy`: Biblioteca de procesamiento del lenguaje natural (PLN). Se la utilizar谩 para 

## [Conexi贸n a la base de datos MongoDB](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=Cqa8eXtVXZQR)
El proyecto se conecta a una base de datos MongoDB utilizando la siguiente configuraci贸n:

```python
uri = "mongodb+srv://<username>:<password>@<cluster_url>/"
client = MongoClient(uri, server_api=ServerApi('1'))
```

El URI de conexi贸n debe reemplazarse con las credenciales y la URL de la base deseada, en este caso se us贸 un cluster de Mongo Atlas.
<br>
Con las siguiente l铆nea de c贸digo obtenemos una confirmaci贸n en caso de conectarse a una base en la nube
```python
# Send a ping to confirm a successful connection
try:
    client.admin.command('ping')
    print("Pinged your deployment. You successfully connected to MongoDB!")
except Exception as e:
    print(e)
```
## Especificaci贸n de la colecci贸n de la base de datos
El proyecto especifica la base de datos y la colecci贸n de MongoDB con la siguiente configuraci贸n:

```python
mongo_db = 'Preprocessing'
mongo_collection = 'tweets'
db = client[mongo_db]
datos = db[mongo_collection].find
```
#### [Colecciones:](https://github.com/jeanpanamito/Proyecto_practicum_IA/tree/main/Archivos)
1. tweets = tweets de muestra Original
2. tweetsOriginals = twets sin rts 
3. tweetsPreprocessed = muestra sin rts con preprocesamiento inicial 
4. tweetsLemmaComparation = tweets sin rts comparacion entre lemmatizaci贸n nltk y spacy 
5. ecuadorTweets = tweets filtrados y preprocesados
 
## [Preprocesamiento](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=omi0VcgIhMpP)
El proyecto realiza varias etapas de preprocesamiento de datos para preparar los textos de los tweets antes de realizar el an谩lisis de sentimientos y la clasificaci贸n.

### [1. Discriminar Retweets](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=akvFu2sjhnCO&line=2&uniqifier=1)
Filtramos y descartamos tweets que son retweets, es decir, aquellos tweets que tienen el mismo texto que otro tweet ya presente en la lista. El objetivo es obtener una lista de tweets que contenga solo los tweets originales sin duplicados. <br>
As铆 evitamos alteraciones en los resultados por tweets que no aportan texto adicional.

### [2. Filtrado por ubicaci贸n](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=mwdTQC2pdlYK&line=1&uniqifier=1)
Se utiliza la expresi贸n `dfProcessed['user'].apply(lambda x: 'ecuador' in x.get('location', '').lower())` para crear una serie booleana que indica si la ubicaci贸n de cada usuario en `dfProcessed` contiene la palabra **"ecuador"** en letras min煤sculas.
La expresi贸n `x.get('location', '')` se utiliza para obtener el valor de la clave **'location'** del diccionario **x**, que representa cada fila en la columna **'user'** del DataFrame. Si no existe **'location'**, se devuelve una cadena vac铆a.
El resultado del filtro se almacena en un nuevo DataFrame llamado **filtered_df**.

### [3. Limpieza de texto](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=nWi8u6whhgn9&line=4&uniqifier=1)
Se realiza una serie de pasos para limpiar el texto de los tweets:
- Remover usuarios
- Remover caract茅res especiales manteniendo acentos.
- Remover URL.
- Remover puntuaciones.
- Remover espacios extra.
- Remover leading/trailing spaces.

### [4. StopWords](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=FpIHQEKslLEj&line=3&uniqifier=1)
Usamos la librer铆a nltk para remover stopwords <br>
Se descargan los recursos para el tokenizador y las palabras vac铆as (stopwords) en espa帽ol utilizando `nltk.download('stopwords')` y `nltk.download('punkt')` <br>
Cargamos la lista de palabras vac铆as en espa帽ol utilizando `stopwords.words('spanish')`.
Se agrega manualmente la palabra 'rt' a la lista de stopwords utilizando `stop_words.extend(['rt'])`. 'rt' generalmente se refiere a "retweet" y a menudo se elimina en an谩lisis de texto.

### [5. Tokenizaci贸n](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=jjCubgkMhjHk&line=4&uniqifier=1)
Los tweets se dividen en palabras o tokens individuales utilizando el tokenizador `word_tokenize` de NLTK. Esto nos permite trabajar con cada palabra por separado en etapas posteriores. Recordar configurar en base al idioma con: `word_tokenize(text, language='spanish')`

### [6. Correcci贸n ortogr谩fica](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=3W22i8mXmz6I&line=7&uniqifier=1)
Se realiza una correcci贸n ortogr谩fica en los tweets utilizando la biblioteca `SpellChecker`. Esto ayuda a corregir posibles errores de escritura y mejorar la precisi贸n del an谩lisis de sentimientos. (No implementada)

### [7. Lemmatizaci贸n](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=aQyuaRnJbHp4)
Se realiza la lematizaci贸n de las palabras para reducir las palabras a su forma base o lema. Esto ayuda a reducir la variabilidad y mejorar la precisi贸n del an谩lisis de sentimientos. <br>
Realizamos pruebas con la librer铆a NLTK y Spacy

### [8. WordCloud](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=1tKzw69LB15N&line=9&uniqifier=1)
Utilizamos la biblioteca matplotlib y wordcloud para crear una nube de palabras. <br> 
Se define una lista llamada `text_list`, donde se almacena el contenido limpio de cada tweet como una cadena de texto. 
Se utiliza una comprensi贸n de listas para recorrer la columna `clean_text` de `tweetDF` **(un DataFrame)** y unir las palabras limpias en cada tweet con un espacio para formar una cadena. <br>
Luego, se crea una cadena de texto llamada `long_string`, que une todas las cadenas de texto de `text_list` en una sola cadena. <br>
Se configura el objeto **WordCloud** con algunas opciones, como el color de fondo, el n煤mero m谩ximo de palabras a mostrar (max_words), el color y el ancho del contorno.<br>
Se genera la nube de palabras utilizando el m茅todo `generate(long_string)` aplicado al objeto **wordcloud**.<br>
Finalmente, la imagen de la nube de palabras se visualiza utilizando el m茅todo `to_image()`. 

## [Modelado de t贸picos](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=2qnst2d3Zpz7)

Durante esta etapa, se pretende recuperar toda la informaci贸n obtenida desde los tweets para encontrar los temas generales y tem谩ticas implicitas que abarquen todo el contenido del texto, para de esa forma ordenar, resumir y tener mejor comprensi贸n sobre el mismo.
Para este proyecto, hemos decidido trabajar bajo el algoritmo de LDA(Latent Dirichlet Allocation).

### Exploraci贸n de par谩metros para el modelo LDA

Se realizo algunos experimentos al momento de definir los valores de Alpha y Beta
- El primero fue usando los valores por defecto, pero la coherencia que obteniamos, era baja.
- El segundo lo realizamos con un programa el cual nos permitia el que el valor de Alpha y Beta se situe en un rango de (0.01, 1, 0.3), dicho resultado lo almacenabamos en un CSV, el cual lo analizamos y escogimos los valores donde cuya coherencia era la m谩s optima, donde el valor que definimos para estos par谩metros fue de 0.01.

De la misma manera, luego de definir los valores de Alpha y Beta, necesitabamos encontrar el n煤mero de t贸picos m谩s favorable para que no exista una duplicaci贸n de temas entre ellos, es por eso que generamos una gr谩fica la cual nos permitia observar el valor de la coherencia junto con el n煤mero de t贸picos.

- El pico de la gr谩fica era con 2 t贸picos, pero fue descartado ya que era un n煤mero muy limitante a relaci贸n con una extensa informaci贸n de los tweets.
- El siguiente m谩s alto, fue 6 t贸picos, el cual lo elegimos, debido a que los temas de cada t贸pico no eran repetitivos y no se intersecaban.

### Selecci贸n y experimentaci贸n con par谩metros 贸ptimos
Al obtener los par谩metros 贸ptimos los cuales eran:
- Alpha: 0.01
- Beta: 0.01
- Diccionario: id2word
- N煤mero de t贸picos: 6

Se procedio a realizar el modelo final LDA:

```python
num_topics = 6

lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=num_topics,
                                           random_state=100,
                                           chunksize=100,
                                           passes=10,
                                           alpha=0.01,
                                           eta=0.01)
```
### Resultado del modelo LDA
Presentamos el resultado del modelo, mediante una gr谩fica usando la libreria de pyLDAvis.
![Topicos](https://github.com/jeanpanamito/Proyecto_practicum_IA/blob/main/pictures/Topicos.png)

Donde nos permitio observar el contenido de temas que ten铆a cada t贸pico y la distancia/diferencia entre t贸picos que existe.

##  [An谩lisis de Sentimientos](https://colab.research.google.com/drive/1CzrbJVRNDXXsiP752i0PNpjq1TyAy-Tm#scrollTo=bkxtqgMBLG5q)

En la presente documentacion, se describe el proceso de an谩lisis de sentimientos realizado utilizando dos modelos de procesamiento de lenguaje natural. El objetivo fue evaluar diferentes enfoques y determinar cu谩l de los modelos proporcionaba los mejores resultados en t茅rminos de clasificaci贸n de sentimientos en textos, espec铆ficamente en tweets.

### Modelos Utilizados

Se seleccionaron los siguientes modelos para realizar el an谩lisis de sentimientos:

1. **VADER (Valence Aware Dictionary and sEntiment Reasoner):** Se trata de un analizador de sentimientos basado en reglas, incluido en la biblioteca NLTK (Natural Language Toolkit). Este modelo utiliza un conjunto de palabras y reglas predefinidas para asignar un puntaje de sentimiento a cada palabra y generar un puntaje general de sentimiento para un texto.

2. **Twitter-roBERTa-base:** Es un modelo basado en transformer, espec铆ficamente dise帽ado para tareas de an谩lisis de sentimientos en tweets. Se utiliza la biblioteca Transformers para cargar y utilizar este modelo preentrenado.

### Proceso de An谩lisis de Sentimientos

El proceso de an谩lisis de sentimientos se llev贸 a cabo de la siguiente manera:

1. **An谩lisis de Sentimientos con VADER:**

   ```python
   import nltk
   from nltk.sentiment import SentimentIntensityAnalyzer

   # Crear una instancia del analizador de sentimientos VADER
   sia = SentimentIntensityAnalyzer()

   datos = db[mongo_collection].find()

   for tweet in datos:
       tweet_text = tweet['full_text']

       # Realizar preprocesamiento del texto
       clean_text = remove_punctuation(remove_rt(remove_numbers(text_lowercase(tweet_text))))

       # Tokenizaci贸n del texto utilizando una expresi贸n regular
       tokens = re.findall(r'\w+', clean_text)

       # Etiquetar el sentimiento de cada token utilizando VADER
       sentiment_scores = [sia.polarity_scores(token)['compound'] for token in tokens]

       # Asignar el sentimiento promedio del texto al campo 'sentiment'
       tweet['sentiment'] = sum(sentiment_scores) / len(sentiment_scores)

       # Presentar los resultados
       print("Texto original:", tweet_text)
       print("Clean Text:", clean_text)
       print("Tokens:", tokens)
       print("Sentimientos:", sentiment_scores)
       print("Sentimiento promedio:", tweet['sentiment'])
       print("------------------------")
   ```

   En este proceso, se utiliz贸 el analizador de sentimientos VADER de NLTK. Se realiz贸 un preprocesamiento del texto para eliminar puntuaci贸n, n煤meros y menciones a retweets. Luego, se tokeniz贸 el texto y se asign贸 un puntaje de sentimiento a cada token utilizando VADER. El sentimiento promedio se asign贸 al campo 'sentiment' en cada documento de tweet.

2. **An谩lisis de Sentimientos con Twitter-roBERTa-base:**

   ```python
   from transformers import AutoModelForSequenceClassification
   from transformers import AutoTokenizer
   import numpy as np
   from scipy.special import softmax

   task = 'sentiment'
   MODEL = f"cardiffnlp/twitter-roberta-base-{task}"

   tokenizer = AutoTokenizer.from_pretrained(MODEL)

   # Cargar el modelo preentrenado
   model = AutoModelForSequenceClassification.from_pretrained(MODEL)
   model.save_pretrained(MODEL)

   text = "Good night "
   text = preprocess(text)
   encoded_input = tokenizer(text, return_tensors='pt')
   output = model(**encoded_input)
   scores = output[0][0].detach().numpy()
   scores = softmax(scores)

   ranking = np.argsort(scores)
   ranking = ranking[::-1]
   for i in range(scores.shape[0]):
       l = labels[ranking[i]]
       s = scores[ranking[i]]
       print(f"{i+1}) {l} {np.round(float(s), 4)}")
   ```

   En este caso, se utiliz贸 el modelo Twitter-roBERTa-base para el an谩lisis de sentimientos en tweets. Se carg贸 el modelo y se utiliz贸 un tokenizer espec铆fico para este modelo. Luego, se realiz贸 el an谩lisis de sentimientos en un texto de ejemplo y se obtuvieron las puntuaciones de sentimiento para cada etiqueta posible.

### Conclusiones

El an谩lisis de sentimientos realizado utilizando los modelos VADER y Twitter-roBERTa-base proporcion贸 resultados interesantes. El enfoque basado en reglas de VADER fue r谩pido y f谩cil de implementar, y mostr贸 una buena capacidad para identificar el sentimiento general en los tweets. Por otro lado, el modelo Twitter-roBERTa-base, basado en transformer, mostr贸 un enfoque m谩s sofisticado y ajustado espec铆ficamente para el an谩lisis de sentimientos en tweets.

En general, ambos modelos fueron 煤tiles para el an谩lisis de sentimientos en textos, pero el modelo Twitter-roBERTa-base demostr贸 una mayor precisi贸n y capacidad para capturar matices en los sentimientos expresados en los tweets. Por lo tanto, se recomienda utilizar este modelo para tareas de an谩lisis de sentimientos en tweets en situaciones donde se requiera un mayor nivel de detalle y precisi贸n.

## Clasificaci贸n de temas

El proyecto utiliza un modelo de clasificaci贸n de temas preentrenado para clasificar los tweets en categor铆as tem谩ticas. El modelo utiliza la arquitectura BERT (Bidirectional Encoder Representations from Transformers) y ha sido entrenado en un conjunto de datos etiquetados. El modelo toma el texto del tweet y lo clasifica en una de las categor铆as tem谩ticas predefinidas.

## Almacenamiento de resultados

Los resultados del an谩lisis de sentimientos y clasificaci贸n de temas se almacenan en la base de datos MongoDB. Cada documento en la colecci贸n "tweets" contiene la informaci贸n original del tweet, el sentimiento calculado y la categor铆a tem谩tica asignada.

## Conclusiones

El proyecto proporciona una manera eficiente de procesar y analizar grandes vol煤menes de datos de Twitter relacionados con la seguridad nacional. El preprocesamiento de datos y el an谩lisis de sentimientos permiten obtener informaci贸n valiosa sobre la opini贸n p煤blica y los temas relevantes en la plataforma. La clasificaci贸n de temas ayuda a organizar y categorizar los tweets, facilitando el an谩lisis y la identificaci贸n de tendencias.

El proyecto se puede adaptar y personalizar seg煤n las necesidades espec铆ficas del usuario, como agregar nuevas categor铆as tem谩ticas o entrenar modelos personalizados. Tambi茅n se pueden aplicar t茅cnicas adicionales, como la detecci贸n de entidades y la extracci贸n de caracter铆sticas, para obtener informaci贸n m谩s detallada de los tweets.

En resumen, el proyecto ofrece una soluci贸n robusta y escalable para el an谩lisis de datos de Twitter relacionados con la seguridad nacional, brindando una comprensi贸n m谩s profunda de las opiniones y temas relevantes en la plataforma.
