# -*- coding: utf-8 -*-
"""PersonalPr1.2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M77QctIT2NTjjebD9uI9ZB-lEIAxk8Zp

# Librerías
"""

!pip install pandas
!pip install tweepy pymongo

!apt-get install -y hunspell libhunspell-dev
!pip install hunspell

!pip install spacy
!python -m spacy download es_core_news_sm

"""# Conexión a la Base"""

#Imports
import tweepy
import nltk
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi

#Uri de conexión con Credenciales
#uri en mongo compass: mongodb+srv://mate01:mcxDZa9yU8aUaK2O@cluster0tweet-gp.hkqaqos.mongodb.net/
uri = "mongodb+srv://mate01:mcxDZa9yU8aUaK2O@cluster0tweet-gp.hkqaqos.mongodb.net/?retryWrites=true&w=majority"

# Create a new client and connect to the server
client = MongoClient(uri, server_api=ServerApi('1'))

# Send a ping to confirm a successful connection
try:
    client.admin.command('ping')
    print("Pinged your deployment. You successfully connected to MongoDB!")
except Exception as e:
    print(e)

client.list_database_names()

db = client["Preprocessing"]
db.list_collection_names()

collection = db['tweets']
collection.find_one().keys()

"""# Carga de Versiones

## Funciones de Preprocesado
"""

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('punkt')
stop_words = stopwords.words('spanish') #Idioma para Stopwords
stop_words.extend(['rt']) #Añadir RT como stopword

def preprocess(text):
    text = text.lower()
    text = re.sub('@[A-Za-z0-9_]+', '', text) #remover usuarios
    text = re.sub('[^a-zA-ZáéíóúÁÉÍÓÚñ. \s]', '', text) #remover caracteres especiales manteniendo acentos
    text = re.sub('htpps://\S+', '', text) #remover url
    text = re.sub('[^\w\s]', '', text)  # remover puntuaciones
    text = re.sub('\s+', ' ', text)  # remover extra espacios
    text = text.strip()  # Remover leading/trailing spaces
    return text

# Opcion de devolverlo como un solo String
def remove_stopwords(text):
    # Split the text into individual words
    words = text.split(' ')
    # Remove stop words from the list of words
    filtered_words = [word for word in words if word not in stop_words]
    # Join the filtered words back into a single string
    filtered_text = ' '.join(filtered_words)
    return filtered_text

def tokenize(text):
    # Tokenizar el texto en palabras individuales
    tokens = word_tokenize(text, language='spanish')

    # Eliminar palabras vacías (stop words)
    stop_words = set(stopwords.words('spanish'))
    tokens = [word for word in tokens if word not in stop_words]

    # Unir las palabras nuevamente en un solo texto
    # preprocessed_text = ' '.join(tokens)

    # return preprocessed_text
    return tokens

"""## Tweets sin copias"""

tweets = collection.find() #Todos los tweets de la muestra original

cantidad_tweets = collection.count_documents({}) #Comprobar cantidad

#Proceso para descartar tweets en que su texto es el mismo (retweets)

tweet_texts = set() #Todos los textos de todos los tweets
no_rt_tweets = [] #Almacenar tweets(documentos) donde el texto sea original

for tweet in tweets:
    tweet_text = tweet['full_text'] #se obtiene el full_text de cada tweet
    if tweet_text not in tweet_texts: #se compara si existe en el set creado
        tweet_texts.add(tweet_text) #añadir texto original
        no_rt_tweets.append(tweet) #añadir objeto completo

print("Tweets totales:" + str(cantidad_tweets))
print("Sin retweets:" + str(len(no_rt_tweets)))

import pandas as pd

data = no_rt_tweets
tweetDF = pd.DataFrame(data) #Dataframe Pandas con tweets sin repetidos

tweetDF.head()

tweetDF.shape

tweetDF.info()

#Colección con tweets sin full_text repetido
collection = db['tweetsOriginals']

# Seleccionar las columnas deseadas
columnas_deseadas = ['_id', 'id', 'full_text', 'user']
dfTweetsoriginals = tweetDF.loc[:, columnas_deseadas]

dfTweetsoriginals.head(5)
#Tweets Originals tiene solo los campos que nos interesan

#Cargar tweets originales en preprocessing.tweetsOriginals

for _, row in dfTweetsoriginals.iterrows():
    document = row.to_dict()
    collection.insert_one(document)

"""## Tweets con Preprocesado Inicial"""

import pandas as pd

collection = db['tweetsOriginals']
tweetsOriginals = collection.find()
dfTweetsProcessed = pd.DataFrame(tweetsOriginals)

dfTweetsProcessed['clean_text'] = dfTweetsProcessed['full_text'].map(lambda x: preprocess(x))
dfTweetsProcessed['clean_text'] = dfTweetsProcessed['clean_text'].map(lambda x: remove_stopwords(x))
dfTweetsProcessed['clean_text'] = dfTweetsProcessed['clean_text'].map(lambda x: tokenize(x))

dfTweetsProcessed.head(5)
#Clean Text es nuestra columna preprocesada

#Cargar tweets procesados iniciales en preprocessing.tweetsPreprocessed
collection = db['tweetsPreprocessed']
for _, row in dfTweetsProcessed.iterrows():
    document = row.to_dict()
    collection.insert_one(document)

"""### Lemmatizado con NLTK"""

import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

def lemmatize_textNLKT(text):
    lemmatizer = WordNetLemmatizer()
    lemmas = [lemmatizer.lemmatize(token) for token in text]
    return lemmas

import pandas as pd

collection = db['tweetsPreprocessed']
tweetsPreprocessed = collection.find()

dfLemmatizeTweets = pd.DataFrame(tweetsPreprocessed)
dfLemmatizeTweets['NLTK'] = dfLemmatizeTweets['clean_text'].map(lambda x: lemmatize_textNLKT(x))
dfLemmatizeTweets.head(5)

dfLemmatizeTweets = dfLemmatizeTweets.drop(['user', 'id'], axis=1)

dfLemmatizeTweets.head(5)

"""### Lemmatizado Spacy"""

import spacy

# Cargar el modelo en español de spaCy
nlp = spacy.load("es_core_news_sm")

def lemmatize_textSpacy(text):
    doc = nlp(text)
    lemmas = [token.lemma_ for token in doc]
    return lemmas

#Aplicar función de Spacy
dfLemmatizeTweets['Spacy'] = dfLemmatizeTweets['clean_text'].apply(lambda x: lemmatize_textSpacy(' '.join(x)))

dfLemmatizeTweets.head(5)

#Insertar documentos en tweetsLemmaComparation para comparación entre NLTK y SPACY
collection = db['tweetsLemmaComparation']
for _, row in dfLemmatizeTweets.iterrows():
    document = row.to_dict()
    collection.insert_one(document)

"""## Tweets Processed"""

collection = db['tweetsPreprocessed']
tweetsPreprocessed = collection.find()
dfProcessed = pd.DataFrame(tweetsPreprocessed)
dfProcessed = dfProcessed.loc[1:, ['id', 'clean_text', 'user']]

dfProcessed['clean_text'] = dfProcessed['clean_text'].apply(lambda x: lemmatize_textSpacy(' '.join(x)))

dfProcessed.head(10)
#Tweets usando preprocesamiento, tokenizado y lemmatizado

"""### Filtrar por Location con mención a 'Ecuador'"""

import pandas as pd

# Tomando el dataframe 'dfProcessed'
filtered_df = dfProcessed[dfProcessed['user'].apply(lambda x: 'ecuador' in x.get('location', '').lower())]

filtered_df.head()

#Eliminar datos innecesarios del user
filtered_df = filtered_df.assign(location=filtered_df['user'].apply(lambda x: x.get('location', ''))).drop('user', axis=1)

filtered_df.head()

#Dataframe a insertar
dfTweetsEcu = filtered_df.loc[1:, ['id', 'clean_text']]
dfTweetsEcu.head()

#Insertar documentos en ecuadorTweets para uso final
collection = db['ecuadorTweets']
for _, row in dfTweetsEcu.iterrows():
    document = row.to_dict()
    collection.insert_one(document)